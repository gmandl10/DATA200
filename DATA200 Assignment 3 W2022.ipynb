{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Trump, Twitter, and Text\n",
    "\n",
    "Due Date: 11:59pm Monday, February 14, 2022   \n",
    "\n",
    "#### Welcome to the third homework assignment of Data 200! In this assignment, we will work with Twitter data in order to analyze Donald Trump's tweets.\n",
    "\n",
    "#### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the homework, we ask that you write your solutions indivi\n",
    "dually. If you do discuss the assignments with others please include their names below.\n",
    "\n",
    "**Collaborators: list collaborators here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Latex, Markdown\n",
    "import re\n",
    "\n",
    "# Ensure that Pandas shows at least 280 characters in columns, so we can see full tweets\n",
    "pd.set_option('max_colwidth', 280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we start\n",
    "\n",
    "All data is made from real-world phenomena, be it the movement of the planets, animal behavior, or human bodies and activities. Working with data always has a bearing back on how human beings know and act in the world. The dataset that you're about to work with in this homework consists of a compilation of President Trump's Tweets. It's important to acknowledge that these Tweets are more than just data -- they're the means by which the President expresses his opinions, performs public and foreign policy, and shapes the lives of people in the US and all over the world. More fundamentally, these Tweets are a powerful form of speech that is particularly significant on the eve of the 2020 US Presidential Election. We recognize that working with this data now, even in the context of a technical exercise, is not a neutral activity and may create difficult feelings in students. We encourage you to observe what you may be experiencing and invite you to consider these dimensions of data science work alongside your technical lessons and we're glad to discuss these issues together in section.\n",
    "\n",
    "### Disclaimer about sns.distplot()\n",
    "This homework was designed for a slightly older version of seaborn, which does not support the new displot method. Instead, in this homework we will heavily rely on distplot (with a t). As you may have noticed in lab 5, use of the distplot function triggers a deprecation warning to notify the user that they should replace all deprecated functions with the updated version. Generally, warnings should not be suppressed but we will do so in this assignment to avoid cluttering.\n",
    "\n",
    "See the seaborn documentation on [distributions](https://seaborn.pydata.org/tutorial/distributions.html) and [functions](https://seaborn.pydata.org/tutorial/function_overview.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to suppress all DeprecationWarnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "tweets = pd.read_csv(\"tweets_01-08-2021.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0\n",
    "\n",
    "There are many ways we could choose to read the President’s tweets. Why might someone be interested in doing data analysis on the President’s tweets? Name a kind of person or institution which might be interested in this kind of analysis. Then, give two reasons why a data analysis of the President's tweets might be interesting or useful for them. Answer in 2-3 sentences.\n",
    "\n",
    "**Type your answer here, replacing this text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Construct a DataFrame called trump containing data from all the tweets stored in _text_. The index of the DataFrame should be the ID of each tweet (looks something like 907698529606541312). It should have these columns:\n",
    "\n",
    "*date: The time the tweet was created encoded as a datetime object.*\n",
    "*device: The source device of the tweet.*\n",
    "*text: The text of the tweet.*\n",
    "*retweet: The retweet count of the tweet.*\n",
    "\n",
    "**Finally, the resulting DataFrame should be sorted by the index.**\n",
    "\n",
    "Hint: You might want to explicitly specify the columns and indices using pd.DataFrame()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = ...\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 Tweet source analysis \n",
    "\n",
    "In the following questions, we are going to find out the charateristics of Trump tweets and the devices used for the tweets.\n",
    "\n",
    "First let's examine the source field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trump['device'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, we see that there are two device types that are more commonly used than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "trump['device'].value_counts().plot(kind=\"bar\")\n",
    "plt.xlabel('device')\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.title(\"Number of Tweets by Source\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (a) \n",
    "\n",
    "Let's extrat the rows with the two major devices \"Twitter for iPhone\" and \"Twitter for Android\", and then replace \"Twitter for iPhone\" by \"iPhone\" and \"Twitteer for Android\" by \"Android\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_device = ...\n",
    "trump_device.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (b) \n",
    "\n",
    "parse the date into six new columns representing \"year\", \"month\", \"day\", \"hour\", \"minute\" and \"second\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (c) \n",
    "\n",
    "Overlay the distributions of Trump's 2 most frequently used web technologies over the years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (d) \n",
    "\n",
    "Please commeent on the pattern(s) from the plot in Question 2 (c). \n",
    "\n",
    "**Type your answer here, replacing this text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (e) \n",
    "\n",
    "Is there a difference between Trump's tweet behavior across these devices? Draw the distribution over hours of the day that Trump tweets on each device for the 2 most commonly used devices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (f) \n",
    "\n",
    "According to this [Verge article](https://www.theverge.com/2017/3/29/15103504/donald-trump-iphone-using-switched-android), Donald Trump switched from an Android to an iPhone sometime in March 2017.\n",
    "\n",
    "Let's see if this information significantly changes our plot. Create a figure similar to your figure from question 2(e), but this time, only use tweets that were tweeted before 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (g) \n",
    "\n",
    "During the campaign, it was theorized that Donald Trump's tweets from Android devices were written by him personally, and the tweets from iPhones were from his staff. Does your figure give support to this theory? What kinds of additional analysis could help support or reject this claim?\n",
    "\n",
    "**Type your answer here, replacing this text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II Sentiment Analysis \n",
    "\n",
    "It turns out that we can use the words in Trump's tweets to calculate a measure of the sentiment of the tweet. For example, the sentence \"I love America!\" has positive sentiment, whereas the sentence \"I hate taxes!\" has a negative sentiment. In addition, some words have stronger positive / negative sentiment than others: \"I love America.\" is more positive than \"I like America.\"\n",
    "\n",
    "We will use the [VADER (Valence Aware Dictionary and sEntiment Reasoner)](https://github.com/cjhutto/vaderSentiment) lexicon to analyze the sentiment of Trump's tweets. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media which is great for our usage.\n",
    "\n",
    "The VADER lexicon gives the sentiment of individual words. Run the following cell to show the first few rows of the lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$:\t-1.5\t0.80623\t[-1, -1, -1, -1, -3, -1, -3, -1, -2, -1]\n",
      "%)\t-0.4\t1.0198\t[-1, 0, -1, 0, 0, -2, -1, 2, -1, 0]\n",
      "%-)\t-1.5\t1.43178\t[-2, 0, -2, -2, -1, 2, -2, -3, -2, -3]\n",
      "&-:\t-0.4\t1.42829\t[-3, -1, 0, 0, -1, -1, -1, 2, -1, 2]\n",
      "&:\t-0.7\t0.64031\t[0, -1, -1, -1, 1, -1, -1, -1, -1, -1]\n",
      "( '}{' )\t1.6\t0.66332\t[1, 2, 2, 1, 1, 2, 2, 1, 3, 1]\n",
      "(%\t-0.9\t0.9434\t[0, 0, 1, -1, -1, -1, -2, -2, -1, -2]\n",
      "('-:\t2.2\t1.16619\t[4, 1, 4, 3, 1, 2, 3, 1, 2, 1]\n",
      "(':\t2.3\t0.9\t[1, 3, 3, 2, 2, 4, 2, 3, 1, 2]\n",
      "((-:\t2.1\t0.53852\t[2, 2, 2, 1, 2, 3, 2, 2, 3, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(''.join(open(\"vader_lexicon.txt\").readlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the lexicon contains emojis too! Each row contains a word and the polarity of that word, measuring how positive or negative the word is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "The creators of VADER describe the tool’s assessment of polarity, or “compound score,” in the following way:\n",
    "“The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.”\n",
    "\n",
    "As you can see, VADER doesn't \"read\" sentences, but works by parsing sentences into words assigning a preset generalized score from their testing sets to each word separately.\n",
    "\n",
    "VADER relies on humans to stabilize its scoring. The creators use Amazon Mechanical Turk, a crowdsourcing survey platform, to train its model. Its training set of data consists of a small corpus of tweets, New York Times editorials and news articles, Rotten Tomatoes reviews, and Amazon product reviews, tokenized using the natural language toolkit (NLTK). Each word in each dataset was reviewed and rated by at least 20 trained individuals who had signed up to work on these tasks through Mechanical Turk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 (a) \n",
    "\n",
    "Please score the sentiment of one of the following words:\n",
    "\n",
    "- police\n",
    "- order\n",
    "- Democrat\n",
    "- Republican\n",
    "- gun\n",
    "- dog\n",
    "- technology\n",
    "- TikTok\n",
    "- security\n",
    "- face-mask\n",
    "- science\n",
    "- climate change\n",
    "- vaccine\n",
    "\n",
    "What score did you give it and why? Can you think of a situation in which this word would carry the opposite sentiment to the one you’ve just assigned?\n",
    "\n",
    "**Type your answer here, replacing this text.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 (b) \n",
    "\n",
    "VADER aggregates the sentiment of words in order to determine the overall sentiment of a sentence, and further aggregates sentences to assign just one aggregated score to a whole tweet or collection of tweets. This is a complex process and if you'd like to learn more about how VADER aggregates sentiment, here is the info at this link.\n",
    "\n",
    "Are there circumstances (e.g. certain kinds of language or data) when you might not want to use VADER? What features of human speech might VADER misrepresent or fail to capture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Question 3 (c) \n",
    "Read **vader_lexicon.txt** into a DataFrame called sent. The index of the DataFrame should be the words in the lexicon. sent should have one column named polarity, storing the polarity of each word.\n",
    "\n",
    "Hint: The pd.read_csv function may help here. Since the file is tab-separated, be sure to set sep='\\t' in your call to pd.read_csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ...\n",
    "sent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get rid of punctuation since it will cause us to fail to match words. Create a new column called **no_punc** in the trump DataFrame to be the lowercased text of each tweet with all punctuation replaced by a single space. We consider punctuation characters to be any character that isn't a Unicode word character or a whitespace character. You may want to consult the Python documentation on regex for this problem.\n",
    "\n",
    "(Why don't we simply remove punctuation instead of replacing with a space? See if you can figure this out by looking at the tweet data.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your regex in punct_re\n",
    "punct_re = r''\n",
    "trump['no_punc'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 (d) \n",
    "\n",
    "Now, let's convert the tweets into what's called a tidy format to make the sentiments easier to calculate. Use the no_punc column of trump to create a table called tidy_format. The index of the table should be the IDs of the tweets, repeated once for every word in the tweet. \n",
    "\n",
    "It has two columns:\n",
    "\n",
    "1. num: The location of the word in the tweet. For example, if the tweet was \"i love america\", then the location of the word \"i\" is 0, \"love\" is 1, and \"america\" is 2.\n",
    "2. word: The individual words of each tweet.\n",
    "\n",
    "As usual, try to avoid using any for loops. Our solution uses a chain of 5 methods on the trump DataFrame, albeit using some rather advanced Pandas hacking.\n",
    "\n",
    "- **Hint 1:** Try looking at the expand argument to pandas' str.split.\n",
    "- **Hint 2:** Try looking at the stack() method.\n",
    "- **Hint 3:** Try looking at the level parameter of the reset_index method.    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_format = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 (e) \n",
    "\n",
    "Now that we have this table in the tidy format, it becomes much easier to find the sentiment of each tweet: we can join the table with the lexicon table.Add a polarity column to the trump table. The polarity column should contain the sum of the sentiment polarity of each word in the text of the tweet.\n",
    "\n",
    "**Hints:**\n",
    "- You will need to merge the tidy_format and sent tables and group the final answer.\n",
    "- If certain words are not found in the sent table, set their polarities to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump['polarity'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a measure of the sentiment of each of his tweets! Note that this calculation is rather basic; you can read over the VADER readme to understand a more robust sentiment analysis.\n",
    "\n",
    "Now, run the cells below to see the most positive and most negative tweets from Trump in your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most negative tweets:')\n",
    "for t in trump.sort_values('polarity').head()['text']:\n",
    "    print('\\n  ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most positive tweets:')\n",
    "for t in trump.sort_values('polarity', ascending=False).head()['text']:\n",
    "    print('\\n  ', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 (f)\n",
    "\n",
    "Read the 5 most positive and 5 most negative tweets. Do you think these tweets are accurately represented by their polarity scores?\n",
    "\n",
    "**Type your answer here, replacing this text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 \n",
    "\n",
    "Now, let's try looking at the distributions of sentiments for tweets containing certain keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (a) \n",
    "\n",
    "In the cell below, create a single plot showing both the distribution of tweet sentiments for tweets containing *nytimes*, as well as the distribution of tweet sentiments for tweets containing *fox*.\n",
    "\n",
    "Be sure to label your axes and provide a title and legend. Be sure to use different colors for fox and nytimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your codes below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (b) \n",
    "\n",
    "Comment on what you observe in the plot above. Can you find another pair of keywords that lead to interesting plots? Describe what makes the plots interesting. \n",
    "\n",
    "**Type your answer here, replacing this text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 \n",
    "\n",
    "Now, let's see whether there's a difference in sentiment for tweets with hashtags and those without."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 (a) \n",
    "\n",
    "First, we'll need to write some regex that can detect whether a tweet contains a hashtag or a link. We say that:\n",
    "\n",
    "- A tweet is a retweet if it has the string 'rt' anywhere in the tweet if it is preceeded and followed by a non-word character (the start and end of the string count as non-word characters).\n",
    "- A tweet has a hashtag if it has the character '#' anywhere in the tweet followed by a letter.\n",
    "- A tweet contains a link or a picture if it has http anywhere in the tweet\n",
    "(You can check out Trump's Twitter for why these criteria are true).\n",
    "\n",
    "In the cell below, assign rt_re to a regex pattern that identifies retweets and hash_link_re to a regex pattern that identifies tweets with hashtags or links.\n",
    "\n",
    "**Hints:**\n",
    "- Be sure to precede your regex pattern with r to make it a raw string (Ex: r'pattern'). To find out more, you can read the first paragraph of the documentation.\n",
    "- You may find using regex word boundaries helpful for one of your patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_re = ...\n",
    "hash_link_re = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 (b) \n",
    "\n",
    "Let's see whether there's a difference in sentiments for tweets with hashtags/links and those without.\n",
    "\n",
    "Note: You will get a UserWarning error when running the below cell. For the purpose of this homework, you can ignore it.\n",
    "\n",
    "Run the cell below to see a distribution of tweet sentiments based on whether a tweet contains a hashtag or link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(trump[trump['text'].str.contains(hash_link_re)]['polarity'],label='hashtag or link');\n",
    "sns.distplot(trump[~trump['text'].str.contains(hash_link_re)]['polarity'],label='no hashtag or link');\n",
    "plt.xlim(-10, 10);\n",
    "plt.ylim(0, 0.4);\n",
    "plt.title('Distribution of Tweet Polarities (hashtag/link vs none)');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the distributions? Answer in 1-2 sentences.\n",
    "\n",
    "**Type your answer here, replacing this text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have finished Assignment 3! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
